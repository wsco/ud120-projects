{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary']\n",
    "#features_list = ['poi'\n",
    "#                 ,'salary'\n",
    "#                 , 'deferral_payments'\n",
    "#                 , 'total_payments'\n",
    "#                 , 'loan_advances'\n",
    "#                 , 'bonus'\n",
    "#                 , 'bonus_salary_ratio'\n",
    "#                 , 'restricted_stock_deffered'\n",
    "#                 , 'deferred_income'\n",
    "#                 , 'total_stock_value'\n",
    "#                 , 'expenses'\n",
    "#                 , 'exercised_stock_options'\n",
    "#                 , 'other'\n",
    "#                 , 'long_term_incentive'\n",
    "#                 , 'restricted_stock'\n",
    "#                 , 'director_fees'\n",
    "#                 , 'to_messages'\n",
    "#                 , 'from_poi_to_this_person'\n",
    "#                 , 'from_poi_to_this_person_percentage'\n",
    "#                 , 'from_messages'\n",
    "#                 , 'from_this_person_to_poi'\n",
    "#                 , 'from_this_person_to_poi_percentage'\n",
    "#                 , 'shared_receipt_with_poi'\n",
    "#                 ] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "#data_dict.pop('Total')\n",
    "#data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters are:  {'dtc__max_depth': 2, 'dtc__criterion': 'gini', 'dtc__min_samples_split': 2, 'dtc__class_weight': 'balanced', 'feature_selection__k': 6, 'dtc__random_state': 42} \n",
      "\n",
      "The  6  features selected and their importances:\n",
      "feature no. 1: bonus_salary_ratio (0.658595952706) (22.1067164085)\n",
      "feature no. 2: shared_receipt_with_poi (0.180270198721) (6.1299573021)\n",
      "feature no. 3: total_stock_value (0.161133848573) (16.8651432616)\n",
      "feature no. 4: exercised_stock_options (0.0) (16.9328653375)\n",
      "feature no. 5: bonus (0.0) (34.2129648303)\n",
      "feature no. 6: salary (0.0) (17.7678544529)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.69      0.81        39\n",
      "        1.0       0.25      0.80      0.38         5\n",
      "\n",
      "avg / total       0.88      0.70      0.76        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "features_list = [\n",
    "                 'poi',\n",
    "                 'salary',\n",
    "                 # 'deferral_payments',\n",
    "                 # 'total_payments',\n",
    "                 # 'loan_advances',\n",
    "                 'bonus',\n",
    "                 'bonus_salary_ratio',\n",
    "                 # 'restricted_stock_deferred',\n",
    "                 # 'deferred_income',\n",
    "                 'total_stock_value',\n",
    "                 # 'expenses',\n",
    "                 'exercised_stock_options',\n",
    "                 # 'other',\n",
    "                 # 'long_term_incentive',\n",
    "                 # 'restricted_stock',\n",
    "                 # 'director_fees',\n",
    "                 # 'to_messages',\n",
    "                 # 'from_poi_to_this_person',\n",
    "                 # 'from_poi_to_this_person_percentage',\n",
    "                 # 'from_messages',\n",
    "                 # 'from_this_person_to_poi',\n",
    "                 'from_this_person_to_poi_percentage',\n",
    "                 'shared_receipt_with_poi'\n",
    "                 ]\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "\n",
    "data_dict.pop(\"TOTAL\")\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "# Bonus-salary ratio\n",
    "for employee, features in data_dict.iteritems():\n",
    "\tif features['bonus'] == \"NaN\" or features['salary'] == \"NaN\":\n",
    "\t\tfeatures['bonus_salary_ratio'] = \"NaN\"\n",
    "\telse:\n",
    "\t\tfeatures['bonus_salary_ratio'] = float(features['bonus']) / float(features['salary'])\n",
    "\n",
    "# from_this_person_to_poi as a percentage of from_messages\n",
    "for employee, features in data_dict.iteritems():\n",
    "\tif features['from_this_person_to_poi'] == \"NaN\" or features['from_messages'] == \"NaN\":\n",
    "\t\tfeatures['from_this_person_to_poi_percentage'] = \"NaN\"\n",
    "\telse:\n",
    "\t\tfeatures['from_this_person_to_poi_percentage'] = float(features['from_this_person_to_poi']) / float(features['from_messages'])\n",
    "\n",
    "# from_poi_to_this_person as a percentage of to_messages\n",
    "for employee, features in data_dict.iteritems():\n",
    "\tif features['from_poi_to_this_person'] == \"NaN\" or features['to_messages'] == \"NaN\":\n",
    "\t\tfeatures['from_poi_to_this_person_percentage'] = \"NaN\"\n",
    "\telse:\n",
    "\t\tfeatures['from_poi_to_this_person_percentage'] = float(features['from_poi_to_this_person']) / float(features['to_messages'])\n",
    "\n",
    "### Impute missing email features to mean\n",
    "email_features = ['to_messages',\n",
    "\t              'from_poi_to_this_person',\n",
    "\t              'from_poi_to_this_person_percentage',\n",
    "\t              'from_messages',\n",
    "\t              'from_this_person_to_poi',\n",
    "\t              'from_this_person_to_poi_percentage',\n",
    "\t              'shared_receipt_with_poi']\n",
    "from collections import defaultdict\n",
    "email_feature_sums = defaultdict(lambda:0)\n",
    "email_feature_counts = defaultdict(lambda:0)\n",
    "\n",
    "for employee, features in data_dict.iteritems():\n",
    "\tfor ef in email_features:\n",
    "\t\tif features[ef] != \"NaN\":\n",
    "\t\t\temail_feature_sums[ef] += features[ef]\n",
    "\t\t\temail_feature_counts[ef] += 1\n",
    "\n",
    "email_feature_means = {}\n",
    "for ef in email_features:\n",
    "\temail_feature_means[ef] = float(email_feature_sums[ef]) / float(email_feature_counts[ef])\n",
    "\n",
    "for employee, features in data_dict.iteritems():\n",
    "\tfor ef in email_features:\n",
    "\t\tif features[ef] == \"NaN\":\n",
    "\t\t\tfeatures[ef] = email_feature_means[ef]\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys=True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a variety of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Potential pipeline steps\n",
    "scaler = MinMaxScaler()\n",
    "select = SelectKBest()\n",
    "dtc = DecisionTreeClassifier()\n",
    "svc = SVC()\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "# Load pipeline steps into list\n",
    "steps = [\n",
    "\t\t # Preprocessing\n",
    "         # ('min_max_scaler', scaler),\n",
    "         \n",
    "         # Feature selection\n",
    "         ('feature_selection', select),\n",
    "         \n",
    "         # Classifier\n",
    "         ('dtc', dtc)\n",
    "         # ('svc', svc)\n",
    "         # ('knc', knc)\n",
    "         ]\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Parameters to try in grid search\n",
    "parameters = dict(\n",
    "                  feature_selection__k=[2, 3, 5, 6], \n",
    "                  dtc__criterion=['gini', 'entropy'],\n",
    "                  # dtc__splitter=['best', 'random'],\n",
    "                  dtc__max_depth=[None, 1, 2, 3, 4],\n",
    "                  dtc__min_samples_split=[1, 2, 3, 4, 25],\n",
    "                  # dtc__min_samples_leaf=[1, 2, 3, 4],\n",
    "                  # dtc__min_weight_fraction_leaf=[0, 0.25, 0.5],\n",
    "                  dtc__class_weight=[None, 'balanced'],\n",
    "                  dtc__random_state=[42]\n",
    "                  # svc__C=[0.1, 1, 10, 100, 1000],\n",
    "                  # svc__kernel=['rbf'],\n",
    "                  # svc__gamma=[0.001, 0.0001]\n",
    "                  # knc__n_neighbors=[1, 2, 3, 4, 5],\n",
    "                  # knc__leaf_size=[1, 10, 30, 60],\n",
    "                  # knc__algorithm=['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "                  )\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create training sets and test sets\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Cross-validation for parameter tuning in grid search \n",
    "sss = StratifiedShuffleSplit(\n",
    "    labels_train,\n",
    "    n_iter = 20,\n",
    "    test_size = 0.5,\n",
    "    random_state = 0\n",
    "    )\n",
    "\n",
    "# Create, fit, and make predictions with grid search\n",
    "gs = GridSearchCV(pipeline,\n",
    "\t              param_grid=parameters,\n",
    "\t              scoring=\"f1\",\n",
    "\t              cv=sss,\n",
    "\t              error_score=0)\n",
    "gs.fit(features_train, labels_train)\n",
    "labels_predictions = gs.predict(features_test)\n",
    "\n",
    "# Pick the classifier with the best tuned parameters\n",
    "clf = gs.best_estimator_\n",
    "print \"\\n\", \"Best parameters are: \", gs.best_params_, \"\\n\"\n",
    "\n",
    "# Print features selected and their importances\n",
    "features_selected=[features_list[i+1] for i in clf.named_steps['feature_selection'].get_support(indices=True)]\n",
    "scores = clf.named_steps['feature_selection'].scores_\n",
    "importances = clf.named_steps['dtc'].feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print 'The ', len(features_selected), \" features selected and their importances:\"\n",
    "for i in range(len(features_selected)):\n",
    "    print \"feature no. {}: {} ({}) ({})\".format(i+1,features_selected[indices[i]],importances[indices[i]], scores[indices[i]])\n",
    "\n",
    "# Print classification report (focus on precision and recall)\n",
    "report = classification_report( labels_test, labels_predictions )\n",
    "print(report)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------\n",
      "--------- GaussianNB\n",
      "-------------------------------------\n",
      "\n",
      "Started at 20:09:05\n",
      "\n",
      "Fitting 10 folds for each of 68 candidates, totalling 680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 680 out of 680 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scale', MaxAbsScaler(copy=True)), ('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=7, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.81320\tPrecision: 0.31606\tRecall: 0.34450\tF1: 0.32967\tF2: 0.33841\n",
      "\tTotal predictions: 15000\tTrue positives:  689\tFalse positives: 1491\tFalse negatives: 1311\tTrue negatives: 11509\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-04c5f74aa9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m \u001b[0mbest_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_estimator_for_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GaussianNB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-04c5f74aa9c7>\u001b[0m in \u001b[0;36mfind_best_estimator_for_grid\u001b[0;34m(classifier, pipe, params, data_set, feature_list, previous_score, previous_estimator)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# modified tester returns the scores for comparison purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reduce_dim__k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{x} features selected by SelectKBest'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reduce_dim__k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "# suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# There was one user (EUGENE) with no data\n",
    "def find_users_with_no_data(scan_data):\n",
    "    no_data_users = []\n",
    "    for ppl, features in scan_data.items():\n",
    "        non_NaN = False\n",
    "        for feature, value in features.items():\n",
    "            if feature != 'poi':\n",
    "                if value != 'NaN':\n",
    "                    non_NaN = True\n",
    "                    break\n",
    "\n",
    "        if not non_NaN:\n",
    "            no_data_users.append(ppl)\n",
    "\n",
    "    return no_data_users\n",
    "\n",
    "\n",
    "# There are no features with NaN set as the value for every user\n",
    "def find_features_with_no_data(scan_data, feature_list):\n",
    "    no_data_features = []\n",
    "    for feature in feature_list:\n",
    "        non_NaN = False\n",
    "        for ppl, features in scan_data.items():\n",
    "            try:\n",
    "                if features[feature] != 'NaN':\n",
    "                    non_NaN = True\n",
    "                    break\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        if not non_NaN:\n",
    "            no_data_features.append(feature)\n",
    "\n",
    "    return no_data_features\n",
    "\n",
    "\n",
    "# Simple function to print out all the data from the data_dict\n",
    "def display_values(data):\n",
    "    # fields_of_interest = ['poi', 'deferral_payments', 'restricted_stock_deferred', 'total_stock_value', 'restricted_stock']\n",
    "    fields_of_interest = ['restricted_stock_deferred']\n",
    "    # persons_of_interest = ['BELFER ROBERT','BHATNAGAR SANJAY']\n",
    "    persons_of_interest = []\n",
    "    if len(persons_of_interest) > 0:\n",
    "        for person in persons_of_interest:\n",
    "            print(person, json.dumps(data[person], indent=4))\n",
    "    else:\n",
    "        for person, values in data.items():\n",
    "            poi = data[person]['poi']\n",
    "            for key, value in values.items():\n",
    "                if key in fields_of_interest and value != 'NaN':\n",
    "                    print(person, poi, key, value)\n",
    "\n",
    "\n",
    "# Function to correct the entries in the data dictionary that were incorrectly set originally\n",
    "def correct_invalid_values(data_to_correct):\n",
    "    # There are some data input errors for BELFER ROBERT and BHATNAGAR SANJAY\n",
    "    # This function corrects those errors back to their original state as per the provided PDF\n",
    "    empty_value = 'NaN'\n",
    "\n",
    "    data_to_correct['BELFER ROBERT']['deferred_income'] = -102500\n",
    "    data_to_correct['BELFER ROBERT']['deferral_payments'] = empty_value\n",
    "    data_to_correct['BELFER ROBERT']['expenses'] = 3285\n",
    "    data_to_correct['BELFER ROBERT']['director_fees'] = 102500\n",
    "    data_to_correct['BELFER ROBERT']['total_payments'] = 3285\n",
    "    data_to_correct['BELFER ROBERT']['exercised_stock_options'] = empty_value\n",
    "    data_to_correct['BELFER ROBERT']['restricted_stock'] = 44093\n",
    "    data_to_correct['BELFER ROBERT']['restricted_stock_deferred'] = -44093\n",
    "    data_to_correct['BELFER ROBERT']['total_stock_value'] = empty_value\n",
    "\n",
    "    data_to_correct['BHATNAGAR SANJAY']['other'] = empty_value\n",
    "    data_to_correct['BHATNAGAR SANJAY']['expenses'] = 137864\n",
    "    data_to_correct['BHATNAGAR SANJAY']['director_fees'] = empty_value\n",
    "    data_to_correct['BHATNAGAR SANJAY']['total_payments'] = 137864\n",
    "    data_to_correct['BHATNAGAR SANJAY']['exercised_stock_options'] = 15456290\n",
    "    data_to_correct['BHATNAGAR SANJAY']['restricted_stock'] = 2604490\n",
    "    data_to_correct['BHATNAGAR SANJAY']['restricted_stock_deferred'] = -2604490\n",
    "    data_to_correct['BHATNAGAR SANJAY']['total_stock_value'] = 15456290\n",
    "\n",
    "    return data_to_correct\n",
    "\n",
    "\n",
    "# Simple function to count how many non-zero values there are for each feature\n",
    "def count_zeros_and_non_zeros(column):\n",
    "    zeros = 0\n",
    "    non_zeros = 0\n",
    "    for entry in column:\n",
    "        if entry == 0:\n",
    "            zeros += 1\n",
    "        else:\n",
    "            non_zeros += 1\n",
    "\n",
    "    return zeros, non_zeros\n",
    "\n",
    "\n",
    "# Create some new features that show the % of emails sent and received by each individual to/from POIs\n",
    "# These numbers may be more predictive than just absolute numbers as all individuals send different amounts of email\n",
    "def create_new_features(existing_data):\n",
    "    for person, features in existing_data.items():\n",
    "        existing_data[person]['from_poi_pct'] = create_pct(features['from_poi_to_this_person'], features['to_messages'])\n",
    "        existing_data[person]['to_poi_pct'] = create_pct(features['from_this_person_to_poi'], features['from_messages'])\n",
    "\n",
    "    return existing_data\n",
    "\n",
    "\n",
    "# Function to turn a number into a percent of a total value; used for feature engineering\n",
    "def create_pct(partial, total):\n",
    "    # Make sure the data is not NaN (if so return 0)\n",
    "    if partial == 'NaN':\n",
    "        partial = 0\n",
    "    if total == 'NaN':\n",
    "        return 0\n",
    "\n",
    "    # If neither value is NAN, then return a percentage as a float value\n",
    "    return (float(partial) / float(total)) * 100.\n",
    "\n",
    "\n",
    "# Simple function to break the features and labels out of the original data dictionary\n",
    "def split_features_from_labels(raw_data, target_features):\n",
    "    new_data = featureFormat(raw_data, target_features, sort_keys=True)\n",
    "    return targetFeatureSplit(new_data)\n",
    "\n",
    "\n",
    "# Use the feature and labels data to try all possible combinations of parameters\n",
    "# Return the best estimator (if it is performing above the baseline, and better than previous entries\n",
    "# Otherwise return the previous estimator and score\n",
    "def find_best_estimator_for_grid(classifier, pipe, params, data_set, feature_list, previous_score, previous_estimator):\n",
    "    print('')\n",
    "    print('-------------------------------------')\n",
    "    print('--------- {classifier}'.format(classifier=classifier))\n",
    "    print('-------------------------------------')\n",
    "    print('')\n",
    "\n",
    "    start = time.time()\n",
    "    print('Started at {time}'.format(time=time.strftime(\"%H:%M:%S\")))\n",
    "    print('')\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    labels, features = split_features_from_labels(data_set, feature_list)\n",
    "\n",
    "    cv = StratifiedShuffleSplit(labels, random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid=params, cv=cv, scoring='f1', n_jobs=1, verbose=1)\n",
    "    grid.fit(features, labels)\n",
    "\n",
    "    best_estimator = grid.best_estimator_\n",
    "\n",
    "    # modified tester returns the scores for comparison purposes\n",
    "    precision, recall, f1 = test_classifier(grid.best_estimator_, data_set, feature_list)\n",
    "    if grid.best_params_.get('reduce_dim__k'):\n",
    "        print('{x} features selected by SelectKBest'.format(x=grid.best_params_['reduce_dim__k']))\n",
    "        for idx, feature in enumerate(feature_list[1:]):\n",
    "            print(\n",
    "                feature,\n",
    "                grid.best_params_['reduce_dim'].scores_[idx],\n",
    "                grid.best_params_['reduce_dim'].get_support()[idx]\n",
    "            )\n",
    "    else:\n",
    "        print('PCA used for dim reduction; no features reportable.')\n",
    "\n",
    "    # return the previous score and estimator if this is not better\n",
    "    new_score = previous_score\n",
    "    new_estimator = previous_estimator\n",
    "\n",
    "    if precision >= 0.3 and recall >= 0.3 and f1 > previous_score:\n",
    "        new_score = f1\n",
    "        new_estimator = best_estimator\n",
    "\n",
    "    print('-------------------------------------')\n",
    "    end = time.time()\n",
    "    log_time_to_complete(classifier, start, end)\n",
    "\n",
    "    return new_estimator, new_score\n",
    "\n",
    "\n",
    "# Simple logger to track how long each classifier is taking to analyze\n",
    "def log_time_to_complete(classifier, start_time, end_time):\n",
    "    print('Took {time} seconds, to find best {classifier}'.format(classifier=classifier, time=end_time-start_time))\n",
    "\n",
    "\n",
    "# Pulling in all features, will select best features automatically in pipeline (either PCA or SelectKBest)\n",
    "features_list = [\n",
    "    'poi',\n",
    "    'salary', 'to_messages', 'deferral_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred',\n",
    "    'deferred_income', 'total_stock_value', 'expenses', 'from_poi_to_this_person', 'exercised_stock_options',\n",
    "    'from_messages', 'other','from_this_person_to_poi', 'long_term_incentive', 'shared_receipt_with_poi',\n",
    "    'restricted_stock', 'director_fees'\n",
    "]\n",
    "\n",
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "####\n",
    "# Previous validation checks done to fix data\n",
    "# outlier exists for \"TOTAL\" row of data\n",
    "####\n",
    "\n",
    "# print(sorted(data_dict))\n",
    "# print(find_users_with_no_data(data_dict))\n",
    "# print(find_features_with_no_data(data_dict, features_list[1:]))\n",
    "\n",
    "####\n",
    "# Remove the invalid outliers completely\n",
    "####\n",
    "outliers = ['TOTAL', 'LOCKHART EUGENE E', 'THE TRAVEL AGENCY IN THE PARK']\n",
    "for outlier in outliers:\n",
    "    data_dict.pop(outlier, 0)\n",
    "\n",
    "####\n",
    "# Fix the erroneous data for 2 entries\n",
    "####\n",
    "# display_values(data_dict)\n",
    "data_dict = correct_invalid_values(data_dict)\n",
    "\n",
    "# Task 3: Create new feature(s)\n",
    "data_dict = create_new_features(data_dict)\n",
    "\n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "labels, features = split_features_from_labels(my_dataset, features_list)\n",
    "best_estimator = None\n",
    "best_score = 0.\n",
    "\n",
    "# Hyperparameter tuning tests\n",
    "# General parameters (all classifiers)\n",
    "n_features = range(2, 19)\n",
    "scalers = [None, MaxAbsScaler()]\n",
    "\n",
    "# KNearestNeighbor parameters\n",
    "n_neighbours = [1, 3, 5, 7, 9, 10, 15, 20, 50]\n",
    "n_neighbor_weights = ['uniform','distance']\n",
    "n_neighbor_p = [1, 2]\n",
    "\n",
    "# Centroid parameters\n",
    "centroid_distances = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n",
    "centroid_shrink = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# DecisionTree parameters\n",
    "tree_criterion = ['entropy', 'gini']\n",
    "tree_splitter = ['random', 'best']\n",
    "tree_min_splits = range(2, 20, 2)\n",
    "tree_max_features = ['sqrt', 'log2', None]\n",
    "\n",
    "# RandomForest parameters\n",
    "forest_estimators = [5, 10, 15, 20, 25]\n",
    "\n",
    "# AdaBoost parameters\n",
    "boost_estimators = [25, 50, 75, 100, 500]\n",
    "\n",
    "####\n",
    "# Now try each different type of classifier to see how they perform independently\n",
    "# Try out Gaussian NB\n",
    "####\n",
    "pipe = Pipeline([\n",
    "    ('scale', MaxAbsScaler()),\n",
    "    ('reduce_dim', PCA(random_state=42)),\n",
    "    ('clf', GaussianNB())\n",
    "])\n",
    "param_grid = [\n",
    "    {\n",
    "        'scale': scalers,\n",
    "        'reduce_dim': [PCA(random_state=42)],\n",
    "        'reduce_dim__n_components': n_features\n",
    "    },\n",
    "    {\n",
    "        'scale': scalers,\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': n_features\n",
    "    }\n",
    "]\n",
    "\n",
    "best_estimator, best_score = find_best_estimator_for_grid('GaussianNB', pipe, param_grid, my_dataset, features_list, best_score, best_estimator)\n",
    "\n",
    "####\n",
    "# Now try KNearestNeighbors\n",
    "####\n",
    "pipe = Pipeline([\n",
    "    ('scale', MaxAbsScaler()),\n",
    "    ('reduce_dim', PCA(random_state=42)),\n",
    "    ('clf', KNeighborsClassifier())\n",
    "])\n",
    "param_grid = [\n",
    "    {\n",
    "        'scale': scalers,\n",
    "        'reduce_dim': [PCA(random_state=42)],\n",
    "        'reduce_dim__n_components': n_features,\n",
    "        'clf__n_neighbors': n_neighbours,\n",
    "        'clf__weights': n_neighbor_weights,\n",
    "        'clf__p': n_neighbor_p\n",
    "    },\n",
    "    {\n",
    "        'scale': scalers,\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': n_features,\n",
    "        'clf__n_neighbors': n_neighbours,\n",
    "        'clf__weights': n_neighbor_weights,\n",
    "        'clf__p': n_neighbor_p\n",
    "    }\n",
    "]\n",
    "\n",
    "best_estimator, best_score = find_best_estimator_for_grid('KNearestNeighbors', pipe, param_grid, my_dataset, features_list, best_score, best_estimator)\n",
    "\n",
    "####\n",
    "#  Now try NearestCentroid\n",
    "####\n",
    "pipe = Pipeline([\n",
    "    ('scale', MaxAbsScaler()),\n",
    "    ('reduce_dim', PCA(random_state=42)),\n",
    "    ('clf', NearestCentroid())\n",
    "])\n",
    "param_grid = [\n",
    "    {\n",
    "        'scale': scalers,\n",
    "        'reduce_dim': [PCA(random_state=42)],\n",
    "        'reduce_dim__n_components': n_features,\n",
    "        'clf__metric': centroid_distances,\n",
    "        'clf__shrink_threshold': centroid_shrink\n",
    "    },\n",
    "    {\n",
    "        'scale': scalers,\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': n_features,\n",
    "        'clf__metric': centroid_distances,\n",
    "        'clf__shrink_threshold': centroid_shrink\n",
    "    }\n",
    "]\n",
    "\n",
    "best_estimator, best_score = find_best_estimator_for_grid('NearestCentroid', pipe, param_grid, my_dataset, features_list, best_score, best_estimator)\n",
    "\n",
    "\n",
    "####\n",
    "#  Now try DecisionTree\n",
    "####\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA(random_state=42)),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(random_state=42)],\n",
    "        'reduce_dim__n_components': n_features,\n",
    "        'clf__criterion': tree_criterion,\n",
    "        'clf__max_features': tree_max_features,\n",
    "        'clf__splitter': tree_splitter,\n",
    "        'clf__min_samples_split': tree_min_splits\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': n_features,\n",
    "        'clf__criterion': tree_criterion,\n",
    "        'clf__max_features': tree_max_features,\n",
    "        'clf__splitter': tree_splitter,\n",
    "        'clf__min_samples_split': tree_min_splits\n",
    "    }\n",
    "]\n",
    "\n",
    "best_estimator, best_score = find_best_estimator_for_grid('DecisionTree', pipe, param_grid, my_dataset, features_list, best_score, best_estimator)\n",
    "\n",
    "\n",
    "####\n",
    "#  Now try RandomForest\n",
    "####\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA(random_state=42)),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(random_state=42)],\n",
    "        'reduce_dim__n_components': n_features,\n",
    "        'clf__n_estimators': forest_estimators,\n",
    "        'clf__max_features': tree_max_features,\n",
    "        'clf__min_samples_split': tree_min_splits\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': n_features,\n",
    "        'clf__n_estimators': forest_estimators,\n",
    "        'clf__max_features': tree_max_features,\n",
    "        'clf__min_samples_split': tree_min_splits\n",
    "    }\n",
    "]\n",
    "\n",
    "# best_estimator, best_score = find_best_estimator_for_grid('RandomForest', pipe, param_grid, my_dataset, features_list, best_score, best_estimator)\n",
    "\n",
    "####\n",
    "#  Now try AdaBoost\n",
    "####\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA(random_state=42)),\n",
    "    ('clf', AdaBoostClassifier(random_state=42))\n",
    "])\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [PCA(random_state=42)],\n",
    "        'reduce_dim__n_components': n_features,\n",
    "        'clf__n_estimators': boost_estimators\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': n_features,\n",
    "        'clf__n_estimators': boost_estimators\n",
    "    }\n",
    "]\n",
    "\n",
    "# best_estimator, best_score = find_best_estimator_for_grid('AdaBoost', pipe, param_grid, my_dataset, features_list, best_score, best_estimator)\n",
    "\n",
    "####\n",
    "# Write out the details of the absolutely best estimator and the score received (f1 value)\n",
    "####\n",
    "print(best_score, best_estimator)\n",
    "\n",
    "####\n",
    "# Now that we know what classifier is most effective, persist it to disk for testing\n",
    "####\n",
    "dump_classifier_and_data(best_estimator, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named wk_functions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aec29a870ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeatureFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetFeatureSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtester\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_classifier_and_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwk_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_for_outliers_95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_for_outliers_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_for_outliers_NaNs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_RandomForest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_AdaBoost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named wk_functions"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../ud120-projects/tools/\")\n",
    "import pickle\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "import pprint\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "from wk_functions import check_for_outliers_95, check_for_outliers_name, check_for_outliers_NaNs, xval_classifier, best_features, tune_RandomForest, tune_AdaBoost, line\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "##Shortened list of features to important ones based on findings from previously running the script\n",
    "poi_label = ['poi']\n",
    "#financial_features = ['salary', 'total_payments', 'loan_advances', 'bonus',\n",
    "#                      'deferred_income', 'total_stock_value',\n",
    "#                      'expenses', 'exercised_stock_options', 'long_term_incentive',\n",
    "#                      'restricted_stock']\n",
    "                      # Removed other\n",
    "#email_features = ['from_poi_to_this_person', 'shared_receipt_with_poi'] \n",
    "                  # Removed email_address\n",
    "\t\t\t\t  \n",
    "financial_features = ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus',\n",
    "                      'restricted_stock_deferred', 'deferred_income', 'total_stock_value',\n",
    "                      'expenses', 'exercised_stock_options', 'long_term_incentive',\n",
    "                      'restricted_stock', 'director_fees']\n",
    "email_features = ['to_messages', 'from_poi_to_this_person', 'from_messages',\n",
    "                  'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "#financial_features = ['salary', 'deferral_payments']\n",
    "#email_features = ['to_messages', 'from_poi_to_this_person']\n",
    "\n",
    "line()\n",
    "features_list = poi_label + financial_features + email_features\n",
    "print('Total Starting Features:', len(features_list))\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project/final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    #data_dict = pd.DataFrame(data_dict)\n",
    "print('Total Data Points:', len(data_dict))\n",
    "\n",
    "NaNs = [0 for i in range(len(features_list))]\n",
    "for i, person in enumerate(data_dict.values()):\n",
    "    for j, feature in enumerate(features_list):\n",
    "        if person[feature] == 'NaN':\n",
    "            NaNs[j] += 1\n",
    "line() # Check for NaNs in each feature\n",
    "print('# NaNs in Each Feature:')\n",
    "for i, feature in enumerate(features_list):\n",
    "    print(feature + ': ', NaNs[i])\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "line()\n",
    "print('95% Percentile Financial Feature Values')\n",
    "line()\n",
    "print('salary/bonus:')\n",
    "check_for_outliers_95('bonus', 'salary', data_dict)\n",
    "    #   LAY KENNETH L 7000000 1072321\n",
    "    #   SKILLING JEFFREY K 5600000 1111258\n",
    "    #   TOTAL 97343619 26704229\n",
    "line()\n",
    "print('exercised stock options/long term incentive:')\n",
    "check_for_outliers_95('exercised_stock_options', 'long_term_incentive', data_dict)\n",
    "    #   LAY KENNETH L 34348384 3600000\n",
    "    #   TOTAL 311764000 48521928\n",
    "line()\n",
    "print('TOTAL is an outlier that sums the rest of the data')\n",
    "print('It should be removed from the dataset')\n",
    "data_dict.pop('TOTAL')\n",
    "\n",
    "line()\n",
    "print('Check employee names:')\n",
    "check_for_outliers_name(data_dict)\n",
    "line()\n",
    "print('THE TRAVEL AGENCY IN THE PARK is not an employee, and should be removed from the list')\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "line()\n",
    "check_for_outliers_NaNs(data_dict, features_list)\n",
    "print('LOCKHART EUGENE E has only NaN values, so he should be removed from the list')\n",
    "data_dict.pop('LOCKHART EUGENE E')\n",
    "\n",
    "line()\n",
    "print('Updated Total Data Points:', len(data_dict))\n",
    "line()\n",
    "\n",
    "# Check number of persons of interest\n",
    "npoi = 0\n",
    "for p in data_dict.values():\n",
    "    if p['poi']:\n",
    "        npoi += 1\n",
    "print (\"Total POIs: \", npoi)\n",
    "print (\"Total Non-POIs: \", len(data_dict) - npoi)\n",
    "line()\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### Comment out below section to score classifiers without the new features\n",
    "## Create new features: to_poi_ratio and from_poi_ratio\n",
    "for person in my_dataset.values():\n",
    "    person['to_poi_ratio'] = 0\n",
    "    person['from_poi_ratio'] = 0\n",
    "    person['blank'] = 0\n",
    "    if float(person['from_messages']) > 0:\n",
    "        person['to_poi_ratio'] = \\\n",
    "            float(person['from_this_person_to_poi']) / float(person['from_messages'])\n",
    "    if float(person['from_messages']) > 0:\n",
    "        person['from_poi_ratio'] = \\\n",
    "            float(person['from_poi_to_this_person']) / float(person['to_messages'])\n",
    "\n",
    "features_list.extend(['to_poi_ratio', 'from_poi_ratio', 'blank'])\n",
    "print(features_list)\n",
    "print('New features created: to_poi_ratio, from_poi_ratio')\n",
    "print('These features calculate the ratio of emails to or from POIs out')\n",
    "print('of total sent or received')\n",
    "line()\n",
    "print('to_poi_ratio: from_this_person_to_poi / from_messages')\n",
    "print('from_poi_ratio: from_poi_to_this_person / to_messages')\n",
    "print('Features list was updated with to_poi_ratio')\n",
    "print('Updated Total Features:', len(features_list) - 1)\n",
    "line()\n",
    "#### End comment section\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "## Use StratifiedShuffleSplit() to try to identify best features for the model\n",
    "sss = StratifiedShuffleSplit(labels, 1000, random_state = 666)\n",
    "print(sss)\n",
    "print('Split Data Set Using StratifiedShuffleSplit for best feature identification')\n",
    "\n",
    "line()    \n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "\n",
    "# Assign variables for classifier comparisons, these were taken from the suggestions in\n",
    "# the choose_your_own algorithm project: AdaBoost and Random Forest\n",
    "\n",
    "print('Begin training AdaBoost and RandomForest Classifiers...')\n",
    "print('Please wait...')\n",
    "print('Training {0} Features...'.format(len(features_list) - 1))\n",
    "line()\n",
    "\n",
    "scores = []\n",
    "ada_accuracy = []\t\n",
    "ada_precision = []\n",
    "ada_recall = []\n",
    "rf_accuracy = []\n",
    "rf_precision = []\n",
    "rf_recall = []\n",
    "\n",
    "for i in range(len(features[0])):\n",
    "    t = time()\n",
    "    sel = SelectKBest(f_classif, k = i + 1)\n",
    "    sel.fit(features, labels)\n",
    "    reduced_features = sel.fit_transform(features, labels)\n",
    "    stop = np.sort(sel.scores_)[::-1][i]\n",
    "    selected_features_list = [f for j, f in enumerate(features_list[1:]) if sel.scores_[j] >= stop]\n",
    "    selected_features_list = ['poi'] + selected_features_list\n",
    "    \n",
    "    # Run metrics on AdaBoost Classifier\n",
    "    ada = AdaBoostClassifier(random_state = 666)\n",
    "    acc, prec, re = xval_classifier(ada, reduced_features, labels, sss)\n",
    "    ada_accuracy.append(round(float(acc), 2))\n",
    "    ada_precision.append(round(float(prec), 2))\n",
    "    ada_recall.append(round(float(re), 2))\n",
    "\n",
    "    # Run metrics on Random Forest Classifier\n",
    "    rf = RandomForestClassifier(random_state = 777)\n",
    "    acc, prec, re = xval_classifier(rf, reduced_features, labels, sss)\n",
    "    rf_accuracy.append(round(float(acc), 2))\n",
    "    rf_precision.append(round(float(prec), 2))\n",
    "    rf_recall.append(round(float(re), 2))\n",
    "    print('Feature:', features_list[i])\n",
    "    print('Time Spent Fitting k = {0}: {1}s'.format(i + 1, round(time() - t, 2)))\n",
    "    print('AdaBoost Metrics: \\nAccuracy: {0}, Precision: {1}, Recall: {2}'.format(ada_accuracy[-1], \n",
    "          ada_precision[-1], ada_recall[-1]))\n",
    "    print('RandomForest Metrics: \\nAccuracy: {0}, Precision: {1}, Recall: {2}'.format(rf_accuracy[-1], \n",
    "          rf_precision[-1], rf_recall[-1]))\n",
    "    scores.append({'k': str(i + 1), 'feature': features_list[i], 'ada_accuracy': ada_accuracy[-1], \n",
    "                  'ada_precision': ada_precision[-1],  'ada_recall': ada_recall[-1], \n",
    "                  'rf_accuracy': rf_accuracy[-1], 'rf_precision': rf_precision[-1], \n",
    "                  'rf_recall': rf_recall[-1]})\n",
    "    if i < len(features[0]):\n",
    "        print('Please continue to wait...')\n",
    "    line()\n",
    "\n",
    "# Average scores from each feature to get an idea of how the classifiers compare overall\n",
    "line()\n",
    "print('Average Scores For Each Classifier:')\n",
    "\n",
    "avg_ada_accuracy = [] \n",
    "avg_ada_precision = [] \n",
    "avg_ada_recall = []\n",
    "avg_rf_accuracy = []\n",
    "avg_rf_precision = []\n",
    "avg_rf_recall = []\n",
    "\n",
    "for k in scores:\n",
    "    avg_ada_accuracy.append(k['ada_accuracy'])\n",
    "    avg_ada_precision.append(k['ada_precision'])\n",
    "    avg_ada_recall.append(k['ada_recall'])\n",
    "    avg_rf_accuracy.append(k['rf_accuracy'])\n",
    "    avg_rf_precision.append(k['rf_precision'])\n",
    "    avg_rf_recall.append(k['rf_recall'])\n",
    "\n",
    "print('Average AdaBoost Accuracy:', round(sum(avg_ada_accuracy) / len(avg_ada_accuracy), 2))\n",
    "print('Average AdaBoost Precision:', round(sum(avg_ada_precision) / len(avg_ada_precision), 2))\n",
    "print('Average AdaBoost Recall:', round(sum(avg_ada_recall) / len(avg_ada_recall), 2))\n",
    "print('Average RandomForest Accuracy:', round(sum(avg_rf_accuracy) / len(avg_rf_accuracy), 2))\n",
    "print('Average RandomForest Precision:', round(sum(avg_rf_precision) / len(avg_rf_precision), 2))\n",
    "print('Average RandomForest Recall:', round(sum(avg_rf_recall) / len(avg_rf_recall), 2))\n",
    "#line()\n",
    "#print('Show score values for each feature')\n",
    "#pprint.pprint(scores)\n",
    "\n",
    "#Create plots to visualize what the scores of each feature look like\n",
    "line()\n",
    "print('Plot metrics for each classifier, showing the values of each k feature')\n",
    "line()\n",
    "ada_df = pd.DataFrame({'ada_accuracy': ada_accuracy, 'ada_precision': ada_precision, \n",
    "         'ada_recall': ada_recall})\n",
    "rf_df = pd.DataFrame({'rf_accuracy': rf_accuracy, 'rf_precision': rf_precision, \n",
    "        'rf_recall': rf_recall})\n",
    "ada_df.plot()\n",
    "rf_df.plot()\n",
    "#plt.show()\n",
    "#Hide plots\n",
    "\n",
    "print('When comparing AdaBoost to RandomForest, AdaBoost has a  higher average recall,')\n",
    "print('and a more stable (but lower) precison.')\n",
    "print('RandomForest has a higher average precision, with more variation, and a slightly ')\n",
    "print('higher accuracy.')\n",
    "print('It appears that overall RandomForest is doing a bit of a better job than AdaBoost.')\n",
    "\n",
    "line()\n",
    "# Use SelectKBest to score and identify the best features in each classifier\n",
    "print('SelectKBest Features:')\n",
    "\n",
    "max_ada_recall = ada_recall.index(max(ada_recall)) + 1\n",
    "max_ada_precision = ada_precision.index(max(ada_precision)) + 1\n",
    "max_rf_recall = rf_recall.index(max(rf_recall)) + 1\n",
    "max_rf_precision = rf_precision.index(max(rf_precision)) + 1\n",
    "\n",
    "bfl_ada_recall, bf_ada_recall = best_features(max_ada_recall, features, labels, features_list)\n",
    "bfl_rf_recall, bf_rf_recall = best_features(max_rf_recall, features, labels, features_list)\n",
    "\n",
    "print('Number of Ada Recall Best Features', len(bfl_ada_recall) - 1)\n",
    "print('Number of RF Recall Best Features:', len(bfl_rf_recall) - 1)\n",
    "print('AdaBoost has many more features that end up on the best features list than RandomForest')\n",
    "\n",
    "line()\n",
    "print('SelectKBest Features - Ada: {0}'.format(len(bfl_ada_recall) - 1))\n",
    "for f in bfl_ada_recall[1:]:\n",
    "    print(f + ' with a score of: {0}'.format(round(sel.scores_[bfl_ada_recall[1:].index(f)], 2)))\n",
    "line()\n",
    "\n",
    "RF = RandomForestClassifier(random_state = 777)\n",
    "RF.fit(bf_rf_recall, labels)\n",
    "print('RF Feature Importance:')\n",
    "print(RF.feature_importances_)\n",
    "\n",
    "print('Best Features - RF:')\n",
    "for f in bfl_rf_recall[1:]:\n",
    "    print(f + ' with a score of: {0}'.format(round(sel.scores_[bfl_rf_recall[1:].index(f)], 2)))\n",
    "line()\n",
    "print('With AdaBoost and RandomForest, there are 3 best features we can examine/tune.')\n",
    "print('Precision for both classifiers is already over .3, however, RandomForest has a much')\n",
    "print('lower average recall')\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "#rf_tuning_parameters = {'n_estimators': [90], 'min_samples_split': [3], 'max_features': [1]}\n",
    "rf_tuning_parameters = {'n_estimators': [125], \n",
    "                        'min_samples_split': [2], 'max_features': [3]}\n",
    "#ada_tuning_parametrers = {'n_estimators': [70], 'learning_rate': [.6]}\n",
    "ada_tuning_parameters = {'n_estimators': [100],\n",
    "                        'learning_rate': [1]}\n",
    "\n",
    "ada_clf = tune_AdaBoost(ada_tuning_parameters, sss, bf_ada_recall, labels)\n",
    "test_classifier(ada_clf, my_dataset, bfl_ada_recall, folds = 1000)\n",
    "#print(xval_classifier(ada_clf, bf_ada_recall, labels, sss))\n",
    "\n",
    "\n",
    "# Tune RandomForest\n",
    "rf_clf = tune_RandomForest(rf_tuning_parameters, sss, bf_rf_recall, labels)\n",
    "test_classifier(rf_clf, my_dataset, bfl_rf_recall, folds = 1000)\n",
    "#print(xval_classifier(rf_clf, bf_rf_recall, labels, sss))\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "#Dump Randomforest \n",
    "dump_classifier_and_data(rf_clf, my_dataset, bfl_rf_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
